<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8"/>
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Transformer - Genghua's Blog</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap">
  <link rel="stylesheet" href="../static/css/style.css"/>
  <style>
    /* Remove white box */
    .content-container, .post-content, .post-item {
      background: transparent !important;
      border: none !important;
      box-shadow: none !important;
    }

    /* Adjust padding */
    .content-container {
      padding: 2rem 1rem;
      max-width: 800px; /* Restrict maximum width for readability */
      margin: 0 auto; /* Center the container horizontally */
      text-align: left; /* Align text to the left */
    }

    .post-content {
      line-height: 1.8; /* Improve readability with better line spacing */
      text-align: left; /* Ensure all text is left-aligned */
      padding: 0; /* Remove padding to make it look more like an article */
    }

    h1, h2 {
      text-align: left; /* Ensure headings are also left-aligned */
      margin-bottom: 1rem; /* Add spacing below headings */
    }

    .post-meta {
      text-align: left; /* Align meta information to the left */
      font-size: 0.9rem; /* Slightly smaller font for meta details */
      margin-bottom: 2rem;
      color: #666; /* Subtle color for meta text */
    }

    .back-link {
      display: block; /* Place back link on its own line */
      text-align: left; /* Align the back link to the left */
      margin-top: 2rem;
      font-size: 0.9rem;
    }

    /* Table of Contents Styles */
    .table-of-contents {
      border-left: 4px solid #ccc;
      padding-left: 1rem;
      margin-bottom: 2rem;
    }

    .table-of-contents h2 {
      font-size: 1.2rem;
      margin-bottom: 1rem;
    }

    .table-of-contents ul {
      list-style: none;
      padding: 0;
    }

    .table-of-contents li {
      margin-bottom: 0.5rem;
      position: relative; /* Needed for the underline effect */
    }

    .table-of-contents a {
      text-decoration: none;
      color: #333;
      transition: color 0.3s ease, transform 0.3s ease;
      position: relative;
    }

    /* Underline effect on hover */
    .table-of-contents a::after {
      content: '';
      position: absolute;
      left: 0;
      bottom: -2px; /* Position the line slightly below the text */
      width: 0;
      height: 2px;
      background-color: #007BFF; /* Default underline color (blue) */
      transition: width 0.3s ease, background-color 0.3s ease; /* Smooth underline and color transitions */
    }

    .table-of-contents a:hover::after {
      width: 100%; /* Expand the line to full width */
      background-color: #FF5722; /* Change underline color to orange on hover */
    }

    .table-of-contents a:hover {
      color: #FF5722; /* Change text color to match the underline */
      transform: scale(1.05); /* Slightly enlarge the text */
    }

    #math-content {
      display: none; /* Hide the content initially */
    }
    img {
      display: block;
      margin: 1rem auto; /* Center the image with margin */
      max-width: 500px; /* Set a smaller maximum width */
      height: auto; /* Maintain aspect ratio */
      border: 1px solid #ddd; /* Optional: Add a subtle border */
      border-radius: 8px; /* Optional: Add rounded corners */
      box-shadow: 0px 2px 6px rgba(0, 0, 0, 0.1); /* Optional: Add shadow */
    }

    .image-description {
      text-align: center; /* Center the description */
      font-size: 0.9rem; /* Slightly smaller font size */
      color: #888888; /* Set to the specified gray color */
      margin-top: 0.5rem; /* Add spacing above the text */
      max-width: 600px; /* Limit the width of the description */
      margin-left: auto; /* Center horizontally */
      margin-right: auto; /* Center horizontally */
      line-height: 1.4; /* Improve readability with line spacing */
    }
  </style>
</head>
<body>
<div class="page-wrapper">
  <header>
    <div class="header-container">
      <a href="../index.html" class="logo">Genghua's Blog</a>
      <nav>
        <a href="../index.html">Home</a>
        <a href="../topics/index.html">Topics</a>
        <a href="../author.html">Author</a>
      </nav>
    </div>
  </header>
  <main>
    <div class="content-container">
      <h1>Transformer Explainations</h1>
      <p class="post-meta">Published on <time>2025-01-05</time> in <a href="../topics/ai.html">AI</a></p>

      <div class="table-of-contents">
        <h2>Table of Contents</h2>
        <ul>
          <li><a href="#Overview">Overview</a></li>
          <li><a href="#theoretical-background">Theoretical Background</a></li>
        </ul>
      </div>

      <div class="post-content">
        <h2 id="Overview">Overview</h2>
        <p>
          Before explaining what a transformer is, let's review RNNs first.
          RNNs can perform well in many tasks, especially with three-dimensional datasets, but their limitations are also very apparent.
        </p>

        <p>
          The Figure 1 shows a simple two-layer LSTM. We can easily observe a disadvantage of RNNs: the network must execute sequentially.
          In an NLP task, it first needs to calculate (Purple Box) and get a representation of the first position (Red Box), then calculate the representation of the second position, and so on.
        </p>

        <img src="./imgs/transformer/transformer-overview-lstm.png" alt="Cross Entropy Illustration" />
        <p class="image-description">Figure 1: Simple two-layer LSTM structure</p>
        
        <p>
          In this case, the neural network cannot fully use the GPU for parallel computing, which is inefficient and results in wasted resources.
          Although RNNs have many variants, such as GRUs and LSTMs, they still require attention mechanisms to address issues like information bottlenecks.
        </p>

        <p>
          Can we completely abandon RNNs for NLP tasks?
        </p>

        

        <p>
          The answer is obviously YES!!! Google researchers published "Attention Is All You Need" in 2017. The title itself answers this question.
        </p>


        <p>
          Well, let's first take a closer look at the transformer strucuter. 
          We can see that it is also an encoder and decoder structure.
          The encoder is outlined in red, and the decor is represented in blue.
        </p>


        <img src="./imgs/transformer/transformer-overview-encode-decode.png" alt="Cross Entropy Illustration" />
        <p class="image-description">Figure 2: Transformer sturcture with encoder and decoder</p>



        <p>
          The first layer is input layer. 
          In NLP task, the input most likely as a text, the first layer is going to devide the input text to many small unit(called token).
        </p>

        <p>
          There are two key differences compared to RNNs.
          First, the transformer uses BPE (Byte Pair Encoding) to divide the input text.
          Second, the transformer incorporates the position of tokens in the sequence, a feature known as positional encoding.
        </p>
        
        <img src="./imgs/transformer/transformer-overview-input.png" alt="Cross Entropy Illustration" />
        <p class="image-description">Figure 3: Transformer sturcture input layer</p>
        
        <p>
          Next is a main component of the transformer, which is a stack of several encoder/decoder blocks. 
        </p>

        <p>
          It is worth noting that the blocks in different layers of the encoder/decoder blocks have exactly the same structure, 
          and with the only difference being their parameters. 
          This is how the transformer achieves a deeper and more expressive model through this stacking approach in NLP.
        </p>

        <img src="./imgs/transformer/transformer-overview-middle.png" alt="Cross Entropy Illustration" />
        <p class="image-description">Figure 3: Transformer sturcture main network</p>

        <p>
          The final layer is simply a linear output layer followed by a softmax function to produce a probability distribution.
          During training, it still uses cross-entropy loss\[H(p, q) = - \sum_{i=1}^{N} p(i) \log q(i)\] to update the model's weights.
        </p>

        <img src="./imgs/transformer/transformer-overview-output.png" alt="Cross Entropy Illustration" />
        <p class="image-description">Figure 4: Transformer sturcture output layer</p>
        

        <p>
          This is the overview of the Transformer structure.
        </p>

      </div>
      <a href="../index.html" class="back-link">&larr; Back to Home</a>
    </div>
  </main>
  <footer>
    <p>Â© 2025 GenghuasBlog</p>
  </footer>
</div>
</body>
</html>


<script type="text/javascript" async
src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>


