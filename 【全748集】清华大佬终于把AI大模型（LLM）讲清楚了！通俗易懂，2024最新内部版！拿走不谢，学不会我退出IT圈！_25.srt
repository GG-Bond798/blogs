
然后接下来就是transformer的一个

67
0:3:7,75 --> 0:3:8,71
主要组成部分

68
0:3:8,71 --> 0:3:11,65
也是后面红框框出的部分

69
0:3:11,65 --> 0:3:15,48
它是由多个encoder或者decoder former block

70
0:3:15,48 --> 0:3:17,24
堆叠而成的

71
0:3:17,24 --> 0:3:17,84
呃

72
0:3:17,84 --> 0:3:20,42
这种block在encoder和decoder之间

73
0:3:20,42 --> 0:3:21,65
会有一些略微不同

74
0:3:21,65 --> 0:3:24,14
对于encoder端不同层的block

75
0:3:24,14 --> 0:3:26,38
以及或者decoder端不同层的block

76
0:3:26,38 --> 0:3:28,45
它们的结构是完全一致的

77
0:3:28,45 --> 0:3:30,63
只是参数上会有不同

78
0:3:31,3 --> 0:3:33,57
transformer也是通过这样一个堆叠的方式

79
0:3:33,57 --> 0:3:35,49
来得到一个更深

80
0:3:35,49 --> 0:3:38,67
然后表达能力更强的一个模型

81
0:3:40,68 --> 0:3:43,58
嗯最后模型的输出层

82
0:3:43,58 --> 0:3:48,38
其实就是一个线性层的变换和一个soft max来输出

83
0:3:48,38 --> 0:3:50,39
一个在此表范围内的概率分布

84
0:3:50,39 --> 0:3:53,92
其实这个和之前RN的输出层是基本一致的

85
0:3:53,92 --> 0:3:55,6
在训练的过程中

86
0:3:55,6 --> 0:3:59,53
我们也是通过在此表这样一个维度计算交叉熵

87
0:3:59,53 --> 0:4:0,64
来计算loss

88
0:4:0,64 --> 0:4:3,2
进而更新模型的一个参数

89
0:4:3,72 --> 0:4:6,5
以上就是我们transformer模型的一个

90
0:4:6,5 --> 0:4:7,79
整体的一个情况

91
0:4:7,79 --> 0:4:10,92
下面我们就逐个部分来一一的进行讲解

